{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80650411-aed6-48cb-aded-bfea03096e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquete de funciones creadas para realizar el procesado, \n",
    "#caracterización e interpolacion de los datos utilizados en la tesina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c7f89-e352-4055-bcdb-397cf25353d7",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#' @title Registrar archivos de origen\n",
    "#' @description Funcion que crea un dataframe con los archivos de los datos de utilizados para crear el dataframe\n",
    "#' @inheritParams get_raw_gpkg\n",
    "#' @inheritParams get_raw_shp\n",
    "#' @inheritParams get_raw_csv\n",
    "#' @param raw_data_id Espera un string con el id del drive del archivo de origen\n",
    "#' @param raw_data_path Espera un string con el id del drive de la ubicacion del archivo de origen\n",
    "#' @param from Espera un dataframe con los archivos de origen agregados antes, si es el primero lo crea vacio por default\n",
    "#' @return Retorna un dataframe que cada fila contiene un archivo de origen\n",
    "#' @details Primero obtiene el dribble del archivo en el drive\n",
    "#' Luego agrega una linea al dataframe con el nombre, id y ubicacion del archivo de origen\n",
    "#' Se debe utilizar de manera recursiva\n",
    "#' @examples from <- add_from_data(raw_data_id, raw_data_path)\n",
    "#' from <- add_from_data(raw_data_id_2, raw_data_path_2, from)\n",
    "def add_from_data(raw_data_id, raw_data_path, from_data=pd.DataFrame(columns=['name', 'id', 'path'])):\n",
    "    #Paquetes\n",
    "    import googledrive\n",
    "    #Obteniendo la metadata del archivo\n",
    "    dl = drive_get(as_id(raw_data_id))\n",
    "    #Armado del df con los datos del archivo que necesitamos para el indice\n",
    "    from_data = from_data.append({'name': dl['name'], 'id': dl['id'], 'path': raw_data_path}, ignore_index=True)\n",
    "    return from_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8297e-8d61-4dc1-9af6-a75eab479e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Obtener datos crudos\n",
    "#' @description Funcion utilizada para obtener los datos crudos desde el drive\n",
    "#' @inheritParams get_raw_shp\n",
    "#' @inheritParams get_raw_csv\n",
    "#' @inheritParams get_raw_gpkg\n",
    "#' @param raw_data_id Espera un string con el id del archivo de origen\n",
    "#' @param temp_path Espera un String con la ruta de la carpeta temporal donde se descargan y crean los distintos archivos\n",
    "#' Se recomineda usar la funcion file.path() para crear correctamente la ruta\n",
    "#' @return Retorna un dribble con la ruta local de los datos crudos\n",
    "#' @details Obtiene el dribble del archivo de origen y lo descarga\n",
    "#' @examples datos <- get_raw_data(raw_data_id,temp_dir)\n",
    "def get_raw_data(raw_data_id, temp_path):\n",
    "    #Paquetes\n",
    "    import googledrive\n",
    "    import tibble\n",
    "    #Obtiene la metadata del archivo\n",
    "    metadata = drive_get(as_id(raw_data_id))\n",
    "    #Carga el path\n",
    "    path = file.path(temp_path, metadata['name'])\n",
    "    #Si no está de manera local descarga el archivo\n",
    "    try:\n",
    "        drive_download(as_id(metadata['id']), path = path, overwrite = False)\n",
    "    except:\n",
    "        #Si el ya esta descargado solo crea el dribble\n",
    "        print(\"El archivo ya esta descargado localmente\")\n",
    "    #Se agrega la ruta local al dribble\n",
    "    dl = as_dribble(as_id(metadata['id']))\n",
    "    dl['local_path'] = path\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065093d8-ebd8-43fa-afd7-ec91889cf870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Obtener archivos Shapefile\n",
    "#' @description Funcion utilizada para obtener los datos crudos en formato shape desde el drive\n",
    "#' @param file Espera un string con el nombre del archivo a descargar\n",
    "#' @param raw_data_path Espera un string con el id de la ubicacion del archivo de origen\n",
    "#' @param temp_path Espera un String con la ruta de la carpeta temporal donde se descargan y crean los distintos archivos\n",
    "#' Se recomineda usar la funcion file.path() para crear correctamente la ruta\n",
    "#' @return Retorna una lista de dos entradas con los datos y los archivos de origen\n",
    "#' @details Primero obtiene los dribbles del archivo de origen, \n",
    "#' Luego lo descarga y arma el data frame con la info de origen\n",
    "#' Por ultimo importa el archivo a un sf object y arma la lista con el sf object y el df de origen\n",
    "#' @examples datos <- get_raw_shp(file, raw_data_path, temp_dir)\n",
    "#' @export\n",
    "def get_raw_shp(file, raw_data_path, temp_path):\n",
    "    #Paquetes\n",
    "    import googledrive\n",
    "    import dplyr\n",
    "    import sf\n",
    "    #Obtiene una lista de todos los archivos que hay en la carpeta del drive con los datos crudos\n",
    "    lista_base = drive_ls(as_id(raw_data_path),recursive = False)\n",
    "    #Filtra cada archivo del shape para obtener el id, lo descarga y arma el dataframe con los datos de origen que luego se utilizara en el indice\n",
    "    #.sph\n",
    "    shp = lista_base[lista_base['name'] == paste0(file,\".shp\")]\n",
    "    shp_dl = get_raw_data(shp['id'],temp_path)\n",
    "    from_data = add_from_data(shp['id'],raw_data_path)\n",
    "    #.shx\n",
    "    shx = lista_base[lista_base['name'] == paste0(file,\".shx\")]\n",
    "    get_raw_data(shx['id'],temp_path)\n",
    "    from_data = add_from_data(shx['id'],raw_data_path,from_data)\n",
    "    #.dbf\n",
    "    dbf = lista_base[lista_base['name'] == paste0(file,\".dbf\")]\n",
    "    get_raw_data(dbf['id'],temp_path)\n",
    "    from_data = add_from_data(dbf['id'],raw_data_path,from_data)\n",
    "    #.prj\n",
    "    prj = lista_base[lista_base['name'] == paste0(file,\".prj\")]\n",
    "    get_raw_data(prj['id'],temp_path)\n",
    "    from_data = add_from_data(prj['id'],raw_data_path,from_data)\n",
    "    #Importa el shapefile descargado a un sf object, arma la lista de doble entrada y lo retorna\n",
    "    data = st_read(shp_dl['local_path'])\n",
    "    output = {'data': data, 'from': from_data}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0302c-fd70-425c-9613-d5d90f54c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Obtener archivos csv\n",
    "#' @description Funcion utilizada para obtener los datos crudos en formato csv desde el drive\n",
    "#' @param file Espera un string con el nombre del archivo a descargar\n",
    "#' @param raw_data_path Espera un string con el id de la ubicacion del archivo de origen\n",
    "#' @param temp_path Espera un String con la ruta de la carpeta temporal donde se descargan y crean los distintos archivos\n",
    "#' Se recomineda usar la funcion file.path() para crear correctamente la ruta\n",
    "#' @return Retorna una lista de dos entradas con los datos y los archivos de origen\n",
    "#' @details Primero obtiene los dribbles del archivo de origen, \n",
    "#' Luego lo descarga y arma el dframe con la info de origen\n",
    "#' Por ultimo importa el archivo a un sf object y arma la lista con el sf object y el df de origen\n",
    "#' @examples datos <- get_raw_shp(file, raw_data_path, temp_dir)\n",
    "#' @export\n",
    "def get_raw_csv(file, raw_data_path, temp_path):\n",
    "    #Paquetes\n",
    "    import googledrive\n",
    "    import dplyr\n",
    "    import sf\n",
    "    #Obtiene una lista de todos los archivos que hay en la carpeta del drive con los datos crudos\n",
    "    lista_base = drive_ls(as_id(raw_data_path),recursive = False)\n",
    "    #Filtra el archivo csv para obtener el id, lo descarga y arma el dataframe con los datos de origen que luego se utilizara en el indice\n",
    "    #.csv\n",
    "    csv = lista_base[lista_base['name'] == paste0(file,\".csv\")]\n",
    "    csv_dl = get_raw_data(csv['id'],temp_path)\n",
    "    from_data = add_from_data(csv['id'],raw_data_path)\n",
    "    #Importa el csv descargado a un sf object, arma la lista de doble entrada y lo retorna\n",
    "    data = st_read(csv_dl['local_path'])\n",
    "    output = {'data': data, 'from': from_data}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1feff2-aea2-4455-952d-9bcf7c1857f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Obtener archivos geopackage\n",
    "#' @description Funcion utilizada para obtener los datos crudos en formato .gpkg desde el drive\n",
    "#' @param file Espera un string con el nombre del archivo a descargar\n",
    "#' @param raw_data_path Espera un string con el id de la ubicacion del archivo de origen\n",
    "#' @param temp_path Espera un String con la ruta de la carpeta temporal donde se descargan y crean los distintos archivos\n",
    "#' Se recomineda usar la funcion file.path() para crear correctamente la ruta\n",
    "#' @return Retorna una lista de dos entradas con los datos y los archivos de origen\n",
    "#' @details Primero obtiene los dribbles del archivo de origen, \n",
    "#' Luego lo descarga y arma el dframe con la info de origen\n",
    "#' Por ultimo importa el archivo a un sf object y arma la lista con el sf object y el df de origen\n",
    "#' @examples datos <- get_raw_shp(file, raw_data_path, temp_dir)\n",
    "#' @export\n",
    "def get_raw_gpkg(file, raw_data_path, temp_path):\n",
    "    #Paquetes\n",
    "    import googledrive\n",
    "    import dplyr\n",
    "    import sf\n",
    "    #Obtiene una lista de todos los archivos que hay en la carpeta del drive con los datos crudos\n",
    "    lista_base = drive_ls(as_id(raw_data_path),recursive = False)\n",
    "    #Filtra el archivo gpkg para obtener el id, lo descarga y arma el dataframe con los datos de origen que luego se utilizara en el indice\n",
    "    #.gpkg\n",
    "    gpkg = lista_base[lista_base['name'] == paste0(file,\".gpkg\")]\n",
    "    gpkg_dl = get_raw_data(gpkg['id'],temp_path)\n",
    "    from_data = add_from_data(gpkg['id'],raw_data_path)\n",
    "    #Importa el gpkg descargado a un sf object, arma la lista de doble entrada y lo retorna\n",
    "    data = st_read(gpkg_dl['local_path'])\n",
    "    output = {'data': data, 'from': from_data}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b476612f-8658-4012-89ec-be0b9d454616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Obtener los datos del perimetro del lote\n",
    "#' @description Funcion utilizada para obtener los datos del perimetro del lote cargado en el drive como gpkg\n",
    "#' @param file Espera un string con el nombre del archivo a descargar\n",
    "#' @param raw_data_path Espera un string con el id de la ubicacion del archivo de origen\n",
    "#' @param temp_path Espera un String con la ruta de la carpeta temporal donde se descargan y crean los distintos archivos\n",
    "#' Se recomineda usar la funcion file.path() para crear correctamente la ruta\n",
    "#' @return Retorna un dataframe con el nombre del archivo, id de origen y la superficie\n",
    "#' @details Primero obtiene los dribbles del archivo del perimetro, \n",
    "#' Luego lo descarga y importa el archivo a un sf object \n",
    "#' Por ultimo arma un dataframe con el nombre del perimetro, id en el drive y la superficie\n",
    "#' @examples get_boundary_data(file, raw_data_path, temp_dir)\n",
    "#' @export\n",
    "def get_boundary_data(file, raw_data_path, temp_path):\n",
    "    # Paquetes\n",
    "    import googledrive\n",
    "    import dplyr\n",
    "    import sf\n",
    "    import pandas as pd  # Importa pandas para trabajar con DataFrames en Python\n",
    "    \n",
    "    # Obtiene una lista de todos los archivos que hay en la carpeta del drive con los datos crudos\n",
    "    lista_base = drive_ls(as_id(raw_data_path), recursive=False)\n",
    "    \n",
    "    # Filtra el archivo gpkg para obtener el id y lo descarga\n",
    "    gpkg = lista_base[lista_base['name'] == file + \".gpkg\"]  # Utiliza concatenación de cadenas en lugar de paste0\n",
    "    gpkg_dl = get_raw_data(gpkg['id'], temp_path)\n",
    "    \n",
    "    # Importa el archivo descargado y arma el dataframe de retorno con el nombre del archivo,\n",
    "    # el id en el drive y la superficie que abarca el perímetro\n",
    "    sup = sf.read_gpkg(gpkg_dl['local_path'])  # Utiliza la función correcta para leer el archivo geoespacial\n",
    "    sup_area = sup.area  # Calcula el área de los polígonos en el GeoDataFrame\n",
    "    output = pd.DataFrame({'bound_file': file + \".gpkg\", 'bound_id': gpkg['id'], 'bound_sup': sup_area[0]})\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83192f2-a740-4215-ba3c-9ef3b44f1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Cargar datos como capas de un gpkg\n",
    "#' @description Funcion que carga el geopackage con cada dato en una capa y lo guarda localmente\n",
    "#' @param data espera un sf object con el dato a guardar\n",
    "#' @param name Espera un string con el nombre de la capa a guardar\n",
    "#' @param from_df espera un dataframe con los datos de los archivos originales donde fué tomado\n",
    "#' Se recomienda usar el dataframe de la lista obtenida con get_raw_shp\n",
    "#' @param temp_path Espera un String con la ruta de la carpeta temporal donde se descargan y crean los distintos archivos\n",
    "#' Se recomineda usar la funcion file.path() para crear correctamente la ruta\n",
    "#' @param data_type espera un String con el tipo de dato a guardar para guardarlo en su correspondiente carpeta con ese nombre\n",
    "#' @return Retorna un sf object pasado como parametro solo para poder usar el pipe, no tiene uso aun\n",
    "#' @details Primero obtiene el directorio donde guardar los datos limpios, si no existe lo crea\n",
    "#' Luego obtiene el indice, si no existe lo crea\n",
    "#' Luego agrega el gpkg como una capa del tipo de dato correspondiente, si no existe el archivo lo crea\n",
    "#' Por ultimo actualiza, elimina los repetidos y guarda el indice localmente\n",
    "#' @examples put_gpkg_local(data, \"SYM_field_01\", from, \"F:/Facultad/Tesina/Data processor/Temp\", \"mapas_rendimiento.gpkg\")\n",
    "#' @export\n",
    "def put_gpkg_local(data, name, from_df, bound_df, temp_path, datatype):\n",
    "    #Paquetes\n",
    "    import dplyr\n",
    "    import sf\n",
    "    #Ruta local donde se guardan los datos ya procesados, si no existe la crea\n",
    "    clean_path = file.path(temp_path,\"clean\")\n",
    "    os.makedirs(clean_path, exist_ok=True)\n",
    "    #Arma la ruta con la ubicación del indice\n",
    "    index_path = file.path(clean_path,\"index.RData\")\n",
    "    #Obtiene el indice\n",
    "    index = pd.DataFrame(columns=['layer', 'type', 'bound_file', 'bound_id', 'bound_sup', 'from'])\n",
    "    try:\n",
    "        index = pd.read_pickle(index_path)\n",
    "    except:\n",
    "        #Si el indice no está creado, lo crea\n",
    "        print(\"Se creo un indice nuevo\")\n",
    "        index.to_pickle(index_path)\n",
    "    #Guarda el archivo localmente, si ya está creado le agrega una capa\n",
    "    path = file.path(clean_path,datatype)\n",
    "    st_write(data,path, layer=name, drive = \"GPKG\", delete_layer = True)\n",
    "    #Actualiza el indice\n",
    "    index = index.append({'layer': name, 'type': datatype, 'bound_file': bound_df['bound_file'], 'bound_id': bound_df['bound_id'], 'bound_sup': bound_df['bound_sup'], 'from.name': from_df['name'], 'from.id': from_df['id'], 'from.path': from_df['path']}, ignore_index=True)\n",
    "    index = index.drop_duplicates()\n",
    "    #Guarda el indice\n",
    "    index.to_pickle(index_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431573c-de71-4850-92d7-7bbab17750a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Medidas de resumen\n",
    "#' @description Funcion que devulve las medidas de resumen de los datos de un gpkg\n",
    "#' @param path Ruta del directorio donde se encuentra el archivo GPKG.\n",
    "#' @param data_path  Nombre del archivo GPKG con los datos.\n",
    "#' @param index_path Nombre del archivo del indice\n",
    "#' Por defecto es \"index.RDS\"\n",
    "#' @param freecor Cantidad de núcleos de CPU a dejar libres al ejecutar la función\n",
    "#' Por defecto es 1\n",
    "#' @param n_col Espera un entero con el numero de la columna a resumir en caso de que tenga más de una (Datos apareados)\n",
    "#' Por defecto es 1\n",
    "#' @param debug Este paramentro se utiliza en un entorno de prueba para cuando se quiera achicar el numero de capas a este valor\n",
    "#' Por defecto es 0\n",
    "#' @return Retorna un dataframe con las medidas de resumen de cada capa\n",
    "#' @details Primero setea los paramentros para realizar el calculo en multiples nucleos\n",
    "#' Luego lista y recorre las capas para obtener la medidas de resumen\n",
    "#' Por ultimo retorna el dataframe con las medidas de cada capa\n",
    "#' @examples summary_gpkg(\"D:/Facultad/Tesina/Temp/clean\",\"Elevacion.gpkg\",freecor = 2)\n",
    "#' @export\n",
    "def summary_gpkg(path, data_path, index_path=\"index.RDS\", seed=None, outlier=True, freecor=1, ncol=1, debug=0):\n",
    "    #Paquetes\n",
    "    import doParallel\n",
    "    import dplyr\n",
    "    import sf\n",
    "    #Del total de nucleos del procesador le resta los indicados y arma el cluster\n",
    "    cor = detectCores()-freecor\n",
    "    cl = makeCluster(cor)\n",
    "    registerDoParallel(cl)\n",
    "    mcaffinity(1:cor)\n",
    "    on.exit(stopCluster(cl))\n",
    "    \n",
    "    #Obtiene una lista de todas las capas del gpkg\n",
    "    layers = st_layers(file.path(path, data_path))\n",
    "    #Cuando debug > 0 entramos en un entorno de prueba donde se achica el numero de layers de acuerdo al valor indicado\n",
    "    if debug > 0 and debug < len(layers['name']):\n",
    "        layers['name'] = layers['name'][0:debug]\n",
    "    \n",
    "    #Obtiene el indice almancenado de manera local\n",
    "    index = pd.read_pickle(file.path(path, index_path))\n",
    "    index = index.drop_duplicates(subset=['layer'])\n",
    "    #Datos para crear el log\n",
    "    #Crea el nombre del log de la corrida\n",
    "    log_name=\"log_summary_gpkg.txt\"\n",
    "    #Ruta local donde se guardan el log, si no existe la crea\n",
    "    log_path = file.path(path,\"log\")\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    err=\"1\"\n",
    "    #Corre los calculos en paralelo con foreach\n",
    "    ret = foreach(layer = layers['name'], .combine = rbind, .packages = c(\"sf\",\"dplyr\")) %dopar%{\n",
    "        try:\n",
    "            #Levanta la capa\n",
    "            #Read----\n",
    "            gpkg = st_read(file.path(path, data_path),layer = layer)\n",
    "            gpkg = gpkg.rename(columns={gpkg.columns[ncol]: 'data'})\n",
    "            gpkg['data'] = gpkg['data'].astype(float)\n",
    "            # Calcular el primer cuartil (Q1) y el tercer cuartil (Q3)\n",
    "            Q1 = np.quantile(gpkg['data'], 0.25)\n",
    "            Q3 = np.quantile(gpkg['data'], 0.75)\n",
    "            # Calcular el rango intercuartílico (IQR)\n",
    "            IQR = Q3-Q1\n",
    "            # Calcular los límites inferior y superior para identificar los outliers\n",
    "            limite_inferior = Q1 - 1.5 * IQR\n",
    "            limite_superior = Q3 + 1.5 * IQR\n",
    "            # Crear un campo logico que identifique si el dato es un outlier\n",
    "            gpkg['outlier'] = np.where((gpkg['data'] < limite_inferior) | (gpkg['data'] > limite_superior), True, False)\n",
    "            #Eliminando los Outliers\n",
    "            if not outlier:\n",
    "                gpkg = gpkg[gpkg['outlier'] == False]\n",
    "            #Calculo de la distancia a todos los puntos\n",
    "            #Si tiene más de 10.000 puntos hace un sample de 10.000 puntos\n",
    "            if len(gpkg) > 10000:\n",
    "                if seed is not None:\n",
    "                    np.random.seed(seed)  # Semilla\n",
    "                gpkg_s = gpkg.sample(n=10000)\n",
    "            else:\n",
    "                gpkg_s = gpkg\n",
    "            #Obtiene desde el indice la superficie de la capa\n",
    "            superficie = index['bound_sup'][index['layer'] == layer]\n",
    "            #Obtiene una matriz con la distancia entre todos los puntos y se guarda el triangulo superior\n",
    "            distancia = st_distance(x=gpkg_s)\n",
    "            y = distancia.toarray()[np.triu_indices(distancia.shape[0])]\n",
    "            x = gpkg['data'].astype(float)\n",
    "            #Dataframe de retorno con todas las medidas\n",
    "            ret = pd.DataFrame({'layer': layer, 'n': len(x), 'media': np.mean(x), 'mediana': np.median(x), 'min': np.min(x), 'max': np.max(x), 'iqr': IQR(x), 'sd': np.std(x), 'var': np.var(x), 'cv': np.std(x)/np.mean(x), 'asim': moments.skewness(x), 'kurt': moments.kurtosis(x), 'dist_media': np.mean(y), 'dens_m': 10000*len(gpkg)/superficie})\n",
    "        except Exception as e:\n",
    "            #si tira un error abre el log, registra la capa y el error, luego lo cierra\n",
    "            with open(file.path(log_path,log_name), 'a') as f:\n",
    "                f.write(\"Error en la capa: \" + layer + \"\\n\")\n",
    "                f.write(err + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c35013-6f9c-46ac-86b4-3107ee660672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#' @title Medidas de resumen de correlación\n",
    "#' @description Funcion que devulve las medidas de resumen de la correlación de un gpkg\n",
    "#' @param path Ruta del directorio donde se encuentra el archivo GPKG.\n",
    "#' @param data_path  Nombre del archivo GPKG con los datos.\n",
    "#' @param freecor Cantidad de núcleos de CPU a dejar libres al ejecutar la función\n",
    "#' Por defecto es 1\n",
    "#' @param n_col Espera un entero con el numero de la columna a resumir en caso de que tenga más de una (Datos apareados)\n",
    "#' Por defecto es 1\n",
    "#' @param debug Este paramentro se utiliza en un entorno de prueba para cuando se quiera achicar el numero de capas a este valor\n",
    "#' Por defecto es 0\n",
    "#' @return Retorna un dataframe con las medidas de resumen de cada capa\n",
    "#' @details Primero setea los paramentros para realizar el calculo en multiples nucleos\n",
    "#' Luego lista y recorre las capas para obtener la medidas de resumen\n",
    "#' Por ultimo retorna el dataframe con las medidas de cada capa\n",
    "#' @examples summary_gpkg(\"D:/Facultad/Tesina/Temp/clean\", \"Elevacion.gpkg\",freecor = 2)\n",
    "#' @export\n",
    "def cor_summary_gpkg(path, data_path, outlier=True, freecor=1, ncol=1, debug=0):\n",
    "    #Paquetes\n",
    "    import doParallel\n",
    "    import dplyr\n",
    "    import sf\n",
    "    import gstat\n",
    "    import tidyverse\n",
    "    cor = detectCores()-freecor\n",
    "    cl = makeCluster(cor)\n",
    "    registerDoParallel(cl)\n",
    "    mcaffinity(1:cor)\n",
    "    on.exit(stopCluster(cl))\n",
    "    \n",
    "    #Obtiene una lista de todas las capas del gpkg\n",
    "    layers = st_layers(file.path(path, data_path))\n",
    "    #Cuando debug > 0 entramos en un entorno de prueba donde se achica el numero de capas de acuerdo al valor indicado\n",
    "    if debug > 0 and debug < len(layers['name']):\n",
    "        layers['name'] = layers['name'][0:debug]\n",
    "    \n",
    "    #Obtiene el indice almancenado de manera local\n",
    "    index = pd.read_pickle(file.path(path, index_path))\n",
    "    index = index.drop_duplicates(subset=['layer'])\n",
    "    #Datos para crear el log\n",
    "    #Crea el nombre del log de la corrida\n",
    "    log_name=\"log_summary_gpkg.txt\"\n",
    "    #Ruta local donde se guardan el log, si no existe la crea\n",
    "    log_path = file.path(path,\"log\")\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    err=\"1\"\n",
    "    #Corre los calculos en paralelo con foreach\n",
    "    ret = foreach(layer = layers['name'], .combine = rbind, .packages = c(\"sf\",\"dplyr\")) %dopar%{\n",
    "        try:\n",
    "            #Levanta la capa\n",
    "            #Read----\n",
    "            gpkg = st_read(file.path(path, data_path),layer = layer)\n",
    "            gpkg = gpkg.rename(columns={gpkg.columns[ncol]: 'data'})\n",
    "            gpkg['data'] = gpkg['data'].astype(float)\n",
    "            # Calcular el primer cuartil (Q1) y el tercer cuartil (Q3)\n",
    "            Q1 = np.quantile(gpkg['data'], 0.25)\n",
    "            Q3 = np.quantile(gpkg['data'], 0.75)\n",
    "            # Calcular el rango intercuartílico (IQR)\n",
    "            IQR = Q3-Q1\n",
    "            # Calcular los límites inferior y superior para identificar los outliers\n",
    "            limite_inferior = Q1 - 1.5 * IQR\n",
    "            limite_superior = Q3 + 1.5 * IQR\n",
    "            # Crear un campo logico que identifique si el dato es un outlier\n",
    "            gpkg['outlier'] = np.where((gpkg['data'] < limite_inferior) | (gpkg['data'] > limite_superior), True, False)\n",
    "            #Eliminando los Outliers\n",
    "            if not outlier:\n",
    "                gpkg = gpkg[gpkg['outlier'] == False]\n",
    "            #Calculo de la distancia a todos los puntos\n",
    "            #Si tiene más de 10.000 puntos hace un sample de 10.000 puntos\n",
    "            if len(gpkg) > 10000:\n",
    "                if seed is not None:\n",
    "                    np.random.seed(seed)  # Semilla\n",
    "                gpkg_s = gpkg.sample(n=10000)\n",
    "            else:\n",
    "                gpkg_s = gpkg\n",
    "            #Obtiene desde el indice la superficie de la capa\n",
    "            superficie = index['bound_sup'][index['layer'] == layer]\n",
    "            #Obtiene una matriz con la distancia entre todos los puntos y se guarda el triangulo superior\n",
    "            distancia = st_distance(x=gpkg_s)\n",
    "            y = distancia.toarray()[np.triu_indices(distancia.shape[0])]\n",
    "            x = gpkg['data'].astype(float)\n",
    "            #Dataframe de retorno con todas las medidas\n",
    "            ret = pd.DataFrame({'layer': layer, 'n': len(x), 'media': np.mean(x), 'mediana': np.median(x), 'min': np.min(x), 'max': np.max(x), 'iqr': IQR(x), 'sd': np.std(x), 'var': np.var(x), 'cv': np.std(x)/np.mean(x), 'asim': moments.skewness(x), 'kurt': moments.kurtosis(x), 'dist_media': np.mean(y), 'dens_m': 10000*len(gpkg)/superficie})\n",
    "        except Exception as e:\n",
    "            #si tira un error abre el log, registra la capa y el error, luego lo cierra\n",
    "            with open(file.path(log_path,log_name), 'a') as f:\n",
    "                f.write(\"Error en la capa: \" + layer + \"\\n\")\n",
    "                f.write(err + \"\\n\")\n",
    "                f.write(str(e) + \"\\n\")\n",
    "    return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
